# Video AI Enhancement Ideas

Videos currently get AI analysis via the thumbnail frame (same as images). AiMetadataGenerationService uses analyzeImage on the medium thumbnail. The prompt is enhanced for video assets: "This is a frame from a video. Analyze it and describe what the video is likely about..."

## Current Implementation

- **Video thumbnails**: Generated by ThumbnailGenerationService (ffmpeg)
- **Video preview**: Generated by VideoPreviewGenerationService (short hover preview)
- **AI analysis**: Uses analyzeImage on the thumbnail frame; prompt enhanced for video context

## Future Enhancement Ideas

### 1. Multi-frame sampling (recommended)

Use ffmpeg to extract 3–5 keyframes at fixed intervals (e.g. 0%, 25%, 50%, 75%, 100% of duration). Run vision on each frame, merge tags, and deduplicate. This gives richer context about what the video contains.

**Implementation sketch:**
- Add `VideoFrameExtractionService` that uses ffmpeg to extract frames to temp storage
- AiMetadataGenerationService: when asset is video, fetch multiple frames, call analyzeImage for each, merge results
- Cost: ~5× API calls per video; consider caching or limiting to first N frames

### 2. Video preview frame sampling

Use the existing preview video URL. Extract frames from the preview (e.g. 3 frames) and analyze. Reuses the preview asset; no extra ffmpeg needed.

### 3. Video-specific metadata

Add video duration, resolution, etc. to the prompt so the AI can infer context (e.g. "30s clip" vs "2min video").

### 4. Native video APIs (future)

OpenAI and others may add native video analysis. When available, switch to video-specific endpoints for full temporal understanding.
